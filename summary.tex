\documentclass[12pt]{article} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
%\usepackage{xcolor}
\usepackage[a4paper,margin=2.5cm]{geometry} 
%\usepackage{setspace}
%\usepackage{float}
%\usepackage{titletoc}
%\usepackage{titlesec}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{mathptmx} \usepackage[bf]{caption} \usepackage{lineno}
\usepackage{url} \usepackage{xpatch} \usepackage{array}
%\usepackage{microtype} \usepackage{relsize}
%\usepackage{footnote}
%\usepackage{tikz}
%\usepackage{ntheorem}
%\usepackage{subcaption}
%\setlength{\parskip}{3pt}
%\usepackage{multicol}
\usepackage{hyperref}
\usepackage{csquotes}
%\usepackage{colortbl}
%\usepackage{pifont}
%\usepackage{multirow}
%\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{pgfplots}
%\usepackage{enumitem}
\usepackage{multicol} 

\usepackage[citestyle=numeric,bibstyle=numeric,sorting=nyt,maxbibnames=9]{biblatex}
\addbibresource{cite.bib} 







%formatting
\setlength\parindent{0pt} % sets indent to zero
\setlength{\parskip}{10pt} % changes vertical space between paragraphs
\renewcommand{\arraystretch}{1.2}



\begin{document}
\begin{titlepage}
\input{titlepage}\clearpage
\end{titlepage}

\pagenumbering{Roman}
\setcounter{page}{2}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\section{Introduction}\label{intro}

Flexible regression is a powerful extension to the classical linear model as it allows to estimate non-linear effects. However, the additional flexibility also introduces more room for errors as the typical bias-variance tradeoff becomes a focal point of estimation. The function can be chosen to be close to the data and therefore have a high variance or a more smooth fit can be chosen increasing the risk of introducing bias by restricting the fit too much. Choosing the "wiggliness" of a fit is at the core of fitting a flexible regression.

An example for a flexible regression is the truncated power basis (TPB), where the dependent variable $y$ is estimated based on the independent variable $x$ using the model equation
$$\hat{y}_i = 
 \theta_0 + \theta_1x_i + \theta_2x_i^2 + ... + \theta_dx_i^d + \sum_{k=1}^K \theta_{dk}(x_i-\kappa_k)_+^d,$$
 
 where $\theta_j$ for $j= 1,...,d$ and $\theta_{dk}$ are the parameters to be estimated and $\kappa_k$ represent knots along the interval $]x_{min},x_{max}[$ which are chosen beforehand \cite{ruppert2003semiparametric, wand2003smoothing}.
 
  A linear TPB with two knots is represented by the model equation
 
 $$\hat{y}_i = 
 \theta_0 + \theta_1x_i +  \theta_{21}(x_i-\kappa_1)_+ + \theta_{22}(x_i-\kappa_2)_+.$$
 
 The parts of the equation that are multiplied with a coefficient are called basis functions. 
An example of a linear TPB can be seen in figure \ref{tpb} with the grey lines depicting the basis functions and the black line the fitted model.
 
This example shows off the fact that the first two basis functions induce a linear fit and the truncated basis functions then add a deviation from that linear fit. Therefore, only the truncated basis functions really introduce wiggliness into the fit. 
Their effect also depends on $i$ as they are zero for $x<\kappa_k$ and only impact the model for parts of the population. 
In order to get a smooth function, it is therefore sensible to penalize the truncated basis functions while leaving the other basis functions unpenalized. This penalty term in the maximum likelihood estimation can be seen as the result of a prior distribution or random effects distribution. 

It follows, that the TPB can be understood as a mixed model where $\theta_j$ are the parameters for the fixed effects and $\theta_{dk}$ the parameters for the random effects \cite{ruppert2003semiparametric}. Representing the TPB as a mixed model introduces the opportunity to use mixed model inference. This is especially useful in determining the wiggliness of a flexible regression fit.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/tpb_report.pdf}
\end{center}
\vspace{-2em}
\caption[caption]{The basis functions of a linear TPB with two knots and a possible fit with $\hat{y}_i = 1 + 1 \cdot x_i +  2 \cdot(x_i-\kappa_1)_+ - 4\cdot (x_i-\kappa_2)_+.$}\label{tpb}
\end{figure}

However, it remains to be proven that this works for other types of flexible regressions as well and not only the TPB. 
In this work, it will be shown that all penalized regression splines can be interpreted as mixed models. The ensuing chapter is dedicated to flexible regression and especially penalized regression splines. Subsequently, a summary of linear mixed models will be presented. Afterwards, a method of representing all penalized regression splines as mixed models is stated followed by some insights into inference for these models. This report closes with a comparison to other methods and a conclusion.


\section{Flexible Regression}

One the most commonly used flexible regression approaches is the \textit{penalized regression spline} which is also the focus of this report \cite{fahrmeir2013regression, ruppert2003semiparametric, wood2017generalized}. In order to introduce this modelling approach, three components are necessary: (flexible) regression, splines, and a penalty. These three parts will be explained in the following.

In nonparametric additive models, the linear parts of the equation are replaced by smooth functions $f_j(.)$ which take the input $\nu_{ij}$, a subset of the features, often just one feature per smooth function, for $j = 1,...,p$ (equation \ref{f}). 
The different smooth functions are then added together, hence the name ``additive model''.
These smooth functions can also be expressed as vectors $v^T_{ij}$  multiplied with their corresponding parameters $\xi_j$ (equation \ref{vector}). These vectors can be merged into matrices when the index $i$ is dropped in order to shorten the notation (equation \ref{matrix}).

\begin{align}
\hat{y}_i &= f_1(\nu_{i1}) + ... + f_p(\nu_{ip})\label{f}\\
&=  v^T_{i1}\xi_1 + ... + v^T_{ip}\xi_p\label{vector} \\
&= V_1\xi_1 + ... + V_p\xi_p = \sum_{j=1}^p V_j\xi_j \label{matrix}
\end{align}

Semiparametric additive models differ from nonparametric ones by including the linear effects, i.\,e., intercept and slope, in a separate term as depicted in equations \ref{f_semi} to \ref{matrix_semi}.

\begin{align}
\hat{y}_i &= f_1(\nu_{i1}) + ... + f_p(\nu_{ip})+ u^T_i\gamma\label{f_semi}\\
&=  v^T_{i1}\xi_1 + ... + v^T_{ip}\xi_p+ u^T_i\gamma\label{vector_semi} \\
&= V_1\xi_1 + ... + V_p\xi_p + U\gamma= \sum_{j=1}^p V_j\xi_j + U\gamma\label{matrix_semi}
\end{align}

In the following, the matrix notation will be used, as it is the most concise.
The report later on focusses on semiparametric regression as opposed to nonparametric regression since it is much easier to see how the method presented in section \ref{representation} needs to be adjusted for a missing linear component than it is vice versa.

After the general model structure is clear, the next step is choosing the matrices $V_j$.
The TPB example in section \ref{intro} already gave an example of a possible choice for that. More generally, the entries in the matrices $V_j$ consist of so-called basis functions. A set of basis functions is called a spline. For univariate semiparametric regression with $l$ basis function, a regression spline equation can be depicted as   

$$\hat{y} = V\xi + U\gamma 
= \begin{pmatrix}
b_1(x_1) & \dots & b_l(x_1) \\
 \vdots  & \ddots & \vdots \\
b_1(x_n) & ... & b_l(x_n)
\end{pmatrix} 
\begin{pmatrix}
\xi_1 \\
 \vdots   \\
\xi_l 
\end{pmatrix} 
+ 
\begin{pmatrix}
1 &  x_1 \\
 \vdots & \vdots \\
1 & x_n
\end{pmatrix} 
\begin{pmatrix}
\gamma_1 \\
\gamma_2 
\end{pmatrix},
$$

with basis functions $b_1(.), ..., b_l(.)$. There are different splines available, for example, natural cubic splines or B-splines. The latter are characterized as peicewise polynomial segments joined together smoothly at knots. The basis functions for a cubic B-spline are depicted in figure \ref{bsplines}. B-splines are nonparametric as they do not include a linear component in their setup. They are a popular choice for basis functions as their basis functions are each only non-zero for a small part of the possible values for $x$, which makes them numerically more stable.

%B-Spline
\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/bspline_report}
\end{center}
\vspace{-2em}
\caption[caption]{Basis functions of a cubic B-spline}\label{bsplines}
\end{figure}


As already mentioned, the bias-variance tradeoff is at the heart of flexible regression. For a given data situation, many possible fits from extremely smooth to very wiggly can be made, two extremes are illustrated in figure \ref{wiggly}. The question is how do we control the bias and variance in a flexible regression setup. So far it can only be controlled by the number of basis functions, which makes adjustments later on really tedious. Instead, the number of basis functions is set as ``enough'' and a penalty is added to the log-likelihood in order to induce shrinkage in the parameters, therefore keeping the wiggliness low. In a semiparametric additive model, the penalty leads to a penalized likelihood as depicted in equation \ref{pen1} with $\lambda$ as the penalty term controlling the intensity of the penalty and the penalty itself being an integral over the curvature, i.\,e., wiggliness or squared second derivative of the fit. As the second derivative of the fit is difficult to determine, a penalty matrix $K$ can be constructed which penalizes the curvature as well leading to equation \ref{pen2}. 

\begin{align}
l_{pen}(\xi,\gamma, \lambda) &=  \log L(\xi, \gamma) + \lambda \int\limits_{x_{min}}^{x_{max}} \left[ f''(x)\right]^2dx \label{pen1}\\
 &= \log L(\xi, \gamma) + \lambda \xi^T K \xi\label{pen2}
\end{align}

The choice of $K$ depends on the spline used. In the case of B-splines, first order differences, with $\xi^TK\xi = \sum(\xi_{k+1} - \xi_{k})^2$, are a popular choice. These exploit the fact that using the same value for consecutive $\xi_j$ will result in a horizontal line representing the maximal penalty. First order differences penalize how much each $\xi_j$ can deviate from the one before, therefore controlling the wiggliness of the fit.

Using a penalized regression spline means the wiggliness of the fit can be easily controlled by adjusting the parameter $lambda$. The remaining problem is how do we determine $\lambda$? This is where mixed models come into play. If we can represent a penalized regression spline as a mixed model, we can use its inference methods to derive an optimal $\lambda$.


\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/wiggly_report}
\end{center}
\vspace{-2em}
\caption[caption]{Bias-variance tradeoff in flexible regression fits: low bias/high variance (dark grey), high bias/low variance (light grey)}\label{wiggly}
\end{figure}



\section{Linear Mixed Models}
both notation with i and matrix notation

Linear mixed models are an extension of the linear model $y_i = x^T_i\beta +\epsilon_i$ incorporating so called \textit{random effects} in order to capture correlation in the data \cite{fahrmeir2013regression}. The correlations might stem from repeated measurements in longitudinal models or clustered data in hierarchical models. In the prediction the fixed or population-averaged effects are modelled separately from the random or cluster-specific effects. The \textit{measurement model} is written as follows:

$$y_i = X_i \beta + Z_i b_i + \epsilon_i,$$

where $i$ refers to an individual in a longitudinal model and to a cluster in a
hierarchical model, $y_i = ( y_{i1},...,y_{in_i} )^T$ is the $n_i \times 1$ vector of responses for the $n_i$ observations for cluster/individual $i$, $X_i = \left[ x_{i1},..., x_{in_i}\right]^T$ is the $n_i \times p$ matrix of the $p$ fixed-effects covariates, $\beta = (\beta_1,...,\beta_p)^T$ is the $p\times1$ vector of parameters for the fixed effects, $Z_i = \left[z_{i1},...,z_{in_i}\right]^T$ is the $n_i\times q$ matrix of $q$ random covariates (often $Z_i$ is a subvector of $X_i$), $b = (b_1,...,b_q)^T$ is the $q\times1$ vector of parameters for the random effects, and $\epsilon_i = (\epsilon_{i1},...,\epsilon_{in_i})^T$ is the $n_i\times1$ vector of errors. 

A linear mixed model can be defined hierarchically in stages: 
In the \textit{first stage} the response is assumed to depend linearly on fixed and random effects as in the measurement model. Additionally, error terms are assumed to be independent and identically distributed (i.i.d.) $\epsilon_i \sim \mathrm{N}_{n_i}(0,R_i)$ typically with $R_i=\sigma^2 I_{n_i}$. 
In the \textit{second stage} a distribution is introduced for the random effects reflecting the assumption that individuals/clusters are drawn from the population.
This distribution can be seen as a Bayesian prior. The convention is to use Gaussian random effects $b_i \sim \mathrm{N}_q(0,D)$ with $(q{+}1)\times (q{+}1)$ covariance matrix $D$. In addition, $b_i$ and $\epsilon_i$ are assumed to be mutually independent. 
Additional correlation structure can be introduced in the error terms $\epsilon_i \sim \mathrm{N}(0, \Sigma_i)$.



The maximum likelihood estimates for the coefficients are $\hat{\beta} = (X^T \hat{V}^{-1} X)^{-1} X^T \hat{V}^{-1}y$ and $\hat{b} = \hat{D}Z^T\hat{V}^{-1} (y-X\hat{\beta})$, with the covariance matrix of observations $y_i$ in cluster $i$ as $V_i = \mathrm{Cov}(y_i) = Z_iDZ_i^T + R_i$ and the covariance matrix for all observations $V = \mathrm{Cov}(y) = \mathrm{diag}(V_1,...V_n)$.
The covariance matrix $V$ is unknown as $R_i$ and $D$ are unknown. $V$ can be estimated  using maximum likelihood (ML) if $\beta$ or its estimate $\hat{\beta}$ is known. An alternative approach is to use the restricted  maximum likelihood  (REML). REML is typically used in linear models to get unbiased estimates. However, it does not generally produce unbiased estimates for mixed models. Nevertheless, it is the method most often used. Here $\beta$ is integrated out of the likelihood corresponding to the Bayesian view of a flat prior on $\beta$. The maximization can be done using Newton-Raphson or Fisher scoring algorithms. 

There are four ways of interpreting a mixed model:
\begin{itemize}
\item \textit{classical view}: The random effects reflect that the individuals/clusters are a random sample of a larger population. This view is not always appropriate, e.\,g., if one has data on all individuals/clusters of the given population.
\item \textit{marginal view}: The random effects generate a general linear
model with correlated errors.
\item \textit{Bayesian view}: The random effects distribution is a prior on the random effects.
\item \textit{penalization view}: The random effects distribution results in a penalty on the random effects under least squares and maximum likelihood estimation inducing to shrinkage towards zero. The intensity of the shrinkage decays with $n$.
\end{itemize}


Several linear mixed models can be differentiated:
Whereas the \textit{random coefficient model} depicted above estimates both slope and intercept, for the \textit{random intercept model} $Z_ib_i$ is replaced by a single random deviation per cluster/individual from the population intercept $b_i \in \mathbb{R}$. The \textit{variance components model} assumes that the components of $b_i$ are independent resulting in a diagonal matrix $D$.
\textit{Multilevel mixed models} have more than one cluster affilitation for each observation and \textit{nested mixed models} as a subgroup of these have hierarchical cluster affiliations.


\section{Semiparametric Models As Mixed Models}\label{representation} % 4 Seiten


Inhalt: 
1. Idee mit empirical bayes approach und ieinem prior für xi
2. Wir sehen, Problem mit rank deficiency, lösung decomposition
3. decomposition mit Bildern erklären
4. Wie kommen wir mit decomposition zu mixed model, requirements (REQ 2 IST PROBLEM)
5. Beweis, dass wir diese Requirements immer erfüllen können
6. So kommen wir zu unserem neuen log-posterior, wir sehen: variance components model
7. Erwaehnen, dass man das auch ohne die Decomposition machen kann

---------------------------------------------------------------

Empirical Bayes


Idea: Use Mixed Model inference: $y = \sum_{j=1}^p \underbrace{V_p \xi_p}_{\text{random effects}} + \overbrace{U \gamma}^{\text{fixed effects}} + \: \epsilon$ 


Prior:  $ p(\gamma) \propto \text{const.}$ 

Prior:   $p(\xi_j|\tau_j^2)  \propto \exp\left(-\frac{1}{2\tau^2_j} \xi_j^T\Sigma^{-1}_j \xi_j\right) $  

Posterior: 
$p(\xi_1, ... \xi_p, \gamma|y) \propto L(y,\xi_1, ... \xi_p, \gamma)\prod_{j=1}^p p(\xi_j|\tau_j^2) $ 

Maximum Likelihood for $\tau_j^2$ (so far treated as fixed):
$$\max_{\tau_1, ..., \tau_p} \log L(\gamma, \xi_1,...,\xi_p) - \sum_{j=1}^p \underbrace{\frac{1}{2\tau^2_j}}_{\lambda_j} \xi_j^T \underbrace{\Sigma^{-1}_j}_{K_j} \xi_j$$




$\Rightarrow$ Empirical Bayes is equivalent to penalized Maximum Likelihood

%Problem: improper priors therefore reparameterize?


Mixed Model Representation
% equivalent to variance components model (components of b_i are independent)

Idea: Use Mixed Model inference: $y = \sum_{j=1}^p \underbrace{V_p \xi_p}_{\text{random effects}} + \overbrace{U \gamma}^{\text{fixed effects}} + \: \epsilon$





Problem: $K_j$ as precision matrix is problematic as $K_j$ is often rank deficient, 
e.\,g. $\xi^TK\xi = \sum(\xi_{k+1} - \xi_{k})^2$ $\rightarrow$ $\xi_1$ not penalized:

The Gaussian prior $p(\xi_j|\tau_j^2) \propto \exp\left(-\frac{1}{2\tau^2_j} \xi_j^TK_j \xi_j\right)$ is improper.




Solution: Separate $\xi_j$ into $\xi_j = \tilde{X}_j \beta_j + \tilde{Z}_j b_j$:

\begin{itemize} 
\item $\beta$: non-penalized parts with a flat prior 
$\text{dim}(\beta_j) = \text{dim}(\xi_j) {-} \text{rank}(K_j)$  

\item $b$: penalized parts with a proper (Gaussian) prior 

$\text{dim}(b_j) =\text{rank}(K_j)$
\end{itemize}


Mixed Model Representation

\begin{figure}
\centering
\hspace{-5.5em}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{Images/priorispenalty_lowvariance_report}
  %\hspace{2em}
\end{minipage}%
\hspace{-2em}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{Images/priorispenalty_highvariance_report}
\end{minipage}
\vspace{-1em}
\caption[caption]{B-Spline basis functions}\label{bsplines}
\end{figure}



Decomposition $\xi_j = \tilde{X}_j \beta_j + \tilde{Z}_j b_j$: 

\begin{align*}
y = \sum_{j=1}^p V_j\\
 \xi_j + U\gamma \\
\overbrace{(\tilde{X}_j \beta_j + \tilde{Z}_j b_j)}^{\xi_j} + U\gamma \\
\overbrace{(\tilde{X}_j \beta_j + \tilde{Z}_j b_j)}^{\xi_j} + U\gamma =   X\beta + Zb 
\end{align*}





\begin{align*}
\beta &:= (\beta_1^T,..., \beta_p^T, \gamma^T) \\
b &:= (b_1^T,...,b_p^T) \\
Z &:= V_j \tilde{Z}_j \\
X &:= (V_j \tilde{X}_j, U)
\end{align*}


Requirements:

\begin{enumerate}
\item 1-to-1 transformation: matrix $(\tilde{X}_j \:\:  \tilde{Z}_j)$ has full rank 
\item $\tilde{X}_j$ and $\tilde{Z}_j$ are orthogonal: $\tilde{X}_j^T \tilde{Z}_j = 0$
\item $\beta_j$ not penalized by $K_j$: $\tilde{X}_j^T K_j \tilde{X}_j = 0$
\item Gaussian prior for $b_j$: $\tilde{Z}_j^T K_j \tilde{Z}_j = I_{kj}$
\end{enumerate}



Choosing $\tilde{X}_j$ and $\tilde{Z}_j$ for Mixed Model Representation


\begin{itemize}
\item $\tilde{X}_j$ is a basis of the null space of $K_j$ (condition 3)
\item $\tilde{Z}_j =L_j (L_j^T L_j)^{-1}$ with $K_j =L_j L_j^T$ (conditions 1 and 4)
\item Choose $L_j$ s.\,t.\ $L_j^T\tilde{X}_j = 0$ and $\tilde{X}_jL_j^T = 0$ (condition 2)

e.\,g.\ spectral decomposition: 
$K_j = \Gamma_j \Lambda_j \Gamma^T$, so $L_j = \Gamma \Lambda_j^{1/2}$
\end{itemize}


Mixed Model Representation

log-Prior:

\begin{align*}
\log p(\xi_j|\tau_j^2)  \propto -\frac{1}{2\tau^2_j} \xi_j^TK_j \xi_j \\
= -\frac{1}{2\tau_j^2} b_j^T b_j
\end{align*}



$\Rightarrow p(\beta) \propto \text{const.}$

$\Rightarrow p(b_j) \sim N(0,\tau_j^2I_{kj})$



log-Posterior:  $$l_p(\beta,b|y) = l(y,\beta,b) - \sum_{j=1}^p \overbrace{\frac{1}{2\tau_j^2}}^{=\lambda} b_j^T  b_j $$

----------------------------------------------------------------




\cite{fahrmeir2013regression}

\cite{kneib2006mixed}

\cite{wood2017generalized}

\cite{wood2011fast}


\section{Inference} % 3 Seiten
Although mixed model inference is not the focus of this report, as it is the reason we are doing the decomposition and stuff, it will be discussed here although rather briefly.

Inhalt
0. das was drüber steht
1. beta und b
2. variance (RE)ML
3. variance estimates mit newton raphson
4. new method mit nested iteration im vergleich zu vorher

------------------------------------------------------------


Estimates $\hat{\beta}$ and $\hat{b}$

In order to maximize the (log-)Posterior (equivalent to ML), derive estimates for $\beta$ and $b$ simultaneously based on known $\sigma^2$ and $\tau^2$.


Mixed Model equations:
$$\overbrace{\begin{pmatrix}
X^TWX & X^TWZ \\
Z^TWX & Z^TWZ + Q^{-1}
\end{pmatrix}}^{\text{Fisher information}}
\begin{pmatrix}
\hat{\beta} \\
\hat{b}
\end{pmatrix} =
\begin{pmatrix}
X^TWy \\
Z^TWy
\end{pmatrix}$$

 with $W=\text{diag}(\sigma^2)$ and $Q=\text{blockdiag}(\tau_1^2I_{k1},..., \tau_p^2I_{kp})$


Variance Estimates

Maximum Likelihood (integrate $b$ out)

\begin{itemize}
\item uses (partially) marginal distribution $y \sim \mathrm{N}(X\beta,\Sigma)$

\begin{enumerate}
\item Derive $\hat{\beta}$ analytically
\item Plug in to get profile likelihood for $\tau^2$ and $\sigma^2$
\item Maximize numerically
\end{enumerate}

\item estimates variance components of posterior mode
\end{itemize}



Restricted ML (integrate $b$ and $\beta$ out)

\begin{itemize}
\item directly uses marginal distribution of $y$

\item Advantages over ML:
\begin{itemize}
\item[$+$] considers loss of degrees of freedom due to estimation of $\beta$ 
\item[$+$] estimates mode of the marginal posterior for the variances
\end{itemize}
\end{itemize}



Variance Estimates $\hat{\sigma}^2$ and $\hat{\tau}^2$

Maximize the restricted likelihood (REML) using Newton-Raphson:

\begin{figure}
\begin{center}
\includegraphics[width=0.55\textwidth]{Images/newton_report}
\end{center}
\vspace{-2em}
\caption[caption]{Newton-Raphson procedure}\label{newton}
\end{figure}






(RE)ML estimation

Single iterations (old)
\begin{itemize}
\item \textbf{Update  $\boldsymbol{\hat{\beta}}$ and $\boldsymbol{\hat{b}}$} given the current $\hat{\lambda}$
\item \textbf{Update $\boldsymbol{\hat{\lambda}}$} using Fisher-Scoring (or Newton-Raphson)

$\rightarrow$ $\mathcal{V}_{\hat{\beta}, \hat{b}}(\lambda)$ depends on $\hat{\beta}$ and $\hat{b}$
\end{itemize}
$\Rightarrow$ Convergence is not guaranteed

Nested iterations (new)
\begin{itemize}
\item Update $\boldsymbol{\hat{\lambda}}$ using Newton-Raphson

\begin{itemize}
\item for each step: estimate $\boldsymbol{\hat{\beta}_\lambda}$ and $\boldsymbol{\hat{b}_\lambda}$
\end{itemize}


$\rightarrow \mathcal{V}(\lambda)$ depends on $\beta$ and $b$ only via $\hat{\beta}_\lambda$ and $\hat{b}_\lambda$

\item Update $\boldsymbol{\hat{\beta}}$ and $\boldsymbol{\hat{b}}$ given the current $\hat{\lambda}$

\end{itemize}
$\Rightarrow$ Convergence is guaranteed (under mild regulatory conditions)


------------------------------------------------------------




\section{Comparison To Other Methods} % eine Seite


Fully Bayesian approach (MCMC)
\begin{itemize}
\item[$+$] no reparameterization needed
\item[$-$] Markov chain convergence is difficult to determine
\item[$-$] how to choose hyperpriors?
\end{itemize}

Prediction error methods (AIC, GCV)
\begin{itemize}
\item[$+$] better prediction error performance 
\item[$-$] worse resistance to overfit
\item[$-$] higher smoothing parameter variability
\item[$-$] increased tendency to multiple minima
\item[$\rightarrow$] \textit{more on that next week}
\end{itemize}


\section{Conclusion} % halbe Seite
Talk about tpr und andere spline methods maybe

\begin{itemize}
\item Semiparametric models can be \textbf{written as mixed models}, which enables an efficient way to estimate the penalty term $\lambda$.
\item In order to get a proper random effects distribution, the flexible parameters have to be \textbf{separated} into sets of parameters with \textbf{flat priors} and sets with \textbf{proper priors}.
\item The penalty term  is proportional to the inverse of the prior variance: $\mathbf{\boldsymbol{\lambda} \boldsymbol{\propto} \frac{1}{\boldsymbol{\tau}^2}}$
\item For good results in mixed model inference, the \textbf{penalty term} has to be estimated in a \textbf{nested iteration} setup with the other parameters.
\end{itemize}


\newpage
\printbibliography

\end{document}