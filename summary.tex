\documentclass[12pt]{article} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
%\usepackage{xcolor}
\usepackage[a4paper,margin=2.5cm]{geometry} 
%\usepackage{setspace}
%\usepackage{float}
%\usepackage{titletoc}
%\usepackage{titlesec}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{mathptmx} \usepackage[bf]{caption} \usepackage{lineno}
\usepackage{url} \usepackage{xpatch} \usepackage{array}
%\usepackage{microtype} \usepackage{relsize}
%\usepackage{footnote}
%\usepackage{tikz}
%\usepackage{ntheorem}
%\usepackage{subcaption}
%\setlength{\parskip}{3pt}
%\usepackage{multicol}
\usepackage{hyperref}
\usepackage{csquotes}
%\usepackage{colortbl}
%\usepackage{pifont}
%\usepackage{multirow}
%\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{pgfplots}
%\usepackage{enumitem}
\usepackage{multicol} 
\usepackage[ruled,vlined]{algorithm2e}

\usepackage[citestyle=numeric,bibstyle=numeric,sorting=nyt,maxbibnames=9]{biblatex}
\addbibresource{cite.bib} 







%formatting
\setlength\parindent{0pt} % sets indent to zero
\setlength{\parskip}{10pt} % changes vertical space between paragraphs
\renewcommand{\arraystretch}{1.2}



\begin{document}
\begin{titlepage}
\input{titlepage}\clearpage
\end{titlepage}

\pagenumbering{Roman}
\setcounter{page}{2}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\section{Introduction}\label{intro}

Flexible regression techniques are a powerful extension to the classical linear regression model as it allows estimating non-linear effects. However, the additional flexibility also introduces more room for errors as the bias-variance trade off becomes a focal point of building a good model. The fitted function can be chosen to be close to the data, which results in a high variance, or a more smooth fit can be selected,  increasing the risk of introducing bias by restricting the fit too much. Deciding on the "wiggliness" of a fit is at the core of fitting a flexible regression model.

An example for a flexible regression model is the truncated power basis (TPB) of degree $d$, where the dependent variable $y_i$ is estimated based on the independent variable $x_i$ for a sample with $i=1,...,n$ using the model equation
$$y_i = 
 \theta_0 + \theta_1x_i + \theta_2x_i^2 + ... + \theta_dx_i^d + \sum_{k=1}^K \theta_{dk}(x_i-\kappa_k)_+^d +\epsilon_i,$$
 
 where $\theta_j$ for $j= 1,...,d$ and $\theta_{dk}$ for $k= 1,...,K$ are the parameters to be estimated and the term $\kappa_k$ represents the $K$ knots along the interval $]x_{min},x_{max}[$, which are chosen beforehand, and the error term is represented by $\epsilon_i$ \cite{ruppert2003semiparametric, wand2003smoothing}.
 
  A TPB of degree one with two knots is represented by the model equation
 
 $$y_i = 
 \theta_0 + \theta_1x_i +  \theta_{21}(x_i-\kappa_1)_+ + \theta_{22}(x_i-\kappa_2)_+ + \epsilon_i.$$
 
 The parts of the equation that are multiplied with a coefficient are called basis functions. 
An example of a linear TPB can be seen in Figure \ref{tpb} with grey lines depicting the basis functions and a black line as the fitted model.
 
This example shows off the fact that the first two basis functions induce a linear fit and the truncated basis functions then add a deviation from that linear fit. Therefore, only the truncated basis functions really introduce wiggliness to the fitted model. 
Their effect also depends on $i$ as they are zero for $x<\kappa_k$ and only impact the fit for parts of the population. 
In order to get a smooth function, it is therefore sensible to penalize the truncated basis functions while leaving the other basis functions unpenalized. This penalty is enforced in the maximum likelihood estimation by adding a term to the log-likelihood. This term is indistinguishable  from the result of a prior distribution or random effects distribution on the respective parameters within the log-likelihood equation. 

It follows, that the TPB can be understood as a mixed model, where the $\theta_j$ are the coefficients for the fixed effects and the $\theta_{dk}$ the coefficients for the random effects \cite{ruppert2003semiparametric}. Representing the TPB as a mixed model introduces the opportunity of using mixed model inference. This, as will be shown later on, is especially useful in determining the wiggliness of a flexible regression fit.

\begin{figure}
\begin{center}
\vspace{2em}
\includegraphics[width=0.82\textwidth]{Images/tpb_report.pdf}
\end{center}
\vspace{-2em}
\caption[caption]{The basis functions of a linear TPB with two knots and a possible fit with $\hat{y}_i = 1 + 1 \cdot x_i +  2 \cdot(x_i{-}\kappa_1)_+ - 4\cdot (x_i{-}\kappa_2)_+.$}\label{tpb}
\end{figure}

However, it remains to be proven that this works for other types of flexible regressions as well and not only the TPB. 
In this work, it will be shown that all penalized regression splines can be interpreted and represented as mixed models. 

The ensuing chapter is dedicated to flexible regression and especially penalized regression splines. Subsequently, a rather mathematical summary of linear mixed models will be presented. Afterwards, an intuitive method of representing all penalized regression splines as mixed models is stated, followed by some insights into inference for these models. This report closes with a comparison to other methods and a conclusion summarizing the main points of this work.


\section{Flexible Regression}

One of the most commonly used flexible regression approaches is the \textit{penalized regression spline}, which is also the focus of this report \cite{fahrmeir2013regression, ruppert2003semiparametric, wood2017generalized}. In order to introduce this modelling approach, understanding its three components is necessary: the (flexible) regression model equations, splines, and the penalty. These three parts will be explained in the following.

\subsection{Model Equations}

In nonparametric additive models, the model equation consists of smooth functions $f_j(.)$, which take the input $\nu_{ij}$, a subset of the features and often just one feature per smooth function, for $j = 1,...,p$ as seen in Equation \ref{f}. 
The different smooth functions are then added together, hence the name ``additive model''.
These smooth functions can also be expressed as vectors $v^T_{ij}$  multiplied with their corresponding parameters $\xi_j$ resulting in Equation \ref{vector}. These vectors in turn can be merged into matrices when the index $i$ is dropped in order to simplify the notation to Equation   \ref{matrix}.

\begin{align}
y_i &= f_1(\nu_{i1}) + ... + f_p(\nu_{ip})+\epsilon_i \label{f}\\
&=  v^T_{i1}\xi_1 + ... + v^T_{ip}\xi_p+\epsilon_i \label{vector} \\
y &= V_1\xi_1 + ... + V_p\xi_p +\epsilon= \sum_{j=1}^p V_j\xi_j +\epsilon \label{matrix}
\end{align}

Semiparametric additive models differ from nonparametric ones by explicitly including a linear component, i.\,e., intercept and slope, in a separate term as depicted in equations \ref{f_semi} to \ref{matrix_semi}.

\begin{align}
y_i &= f_1(\nu_{i1}) + ... + f_p(\nu_{ip})+ u^T_i\gamma+\epsilon_i\label{f_semi}\\
&=  v^T_{i1}\xi_1 + ... + v^T_{ip}\xi_p+ u^T_i\gamma+\epsilon_i\label{vector_semi} \\
y&= V_1\xi_1 + ... + V_p\xi_p + U\gamma +\epsilon= \sum_{j=1}^p V_j\xi_j + U\gamma+\epsilon\label{matrix_semi}
\end{align}

In the following, the matrix notation will be used, as it is the most concise.
The report later on focusses on semiparametric regression as opposed to nonparametric regression, since it is much easier to deduce how the method presented in Section \ref{representation} needs to be adjusted for a missing linear component than it is vice versa.

\subsection{Splines}



After the general model structure is clear, the next step is choosing the matrices inducing the flexibility of the fit, namely $V_j$.
The illustration of the TPB in Section \ref{intro} already gave an example of a possible choice for that matrix. More generally, the rows in the matrices $V_j$ consist of different basis functions. A set of basis functions is called a spline. For univariate semiparametric regression with $l$ basis function, a regression spline equation can be written as   

$$y = V\xi + U\gamma +\epsilon
= \begin{pmatrix}
b_1(x_1) & \dots & b_l(x_1) \\
 \vdots  & \ddots & \vdots \\
b_1(x_n) & ... & b_l(x_n)
\end{pmatrix} 
\begin{pmatrix}
\xi_1 \\
 \vdots   \\
\xi_l 
\end{pmatrix} 
+ 
\begin{pmatrix}
1 &  x_1 \\
 \vdots & \vdots \\
1 & x_n
\end{pmatrix} 
\begin{pmatrix}
\gamma_1 \\
\gamma_2 
\end{pmatrix} +\epsilon,
$$



with basis functions $b_1(.), ..., b_l(.)$. There are different splines available, for example, natural cubic splines or B-splines. The latter are characterized as peicewise polynomial segments joined together smoothly at knots. The basis functions for a cubic B-spline are depicted in Figure \ref{bsplines}. B-splines are nonparametric as they do not include a linear component in their setup. They are a popular choice as their basis functions are each only non-zero for a small part of the possible values for $x$, which makes them numerically more stable.

%B-Spline
\begin{figure}[t]
\begin{center}
\vspace{2em}
\includegraphics[width=0.62\textwidth]{Images/bspline_report}
\end{center}
\vspace{-2em}
\caption[caption]{Basis functions of a cubic B-spline}\label{bsplines}
\end{figure}


\subsection{Penalty}\label{prs}

As already mentioned, the bias-variance trade off is at the heart of flexible regression. For a given data situation, many possible fits from extremely smooth to very wiggly can be made. Two extremes are illustrated in Figure \ref{wiggly}. The question is how do we control the bias and variance in a flexible regression setup. So far it can only be controlled by the number of basis functions, which makes adjustments later on really tedious. Instead, the number of basis functions is set as ``enough'' and a penalty is added to the log-likelihood in order to keep the wiggliness low. In a semiparametric additive model, the penalty leads to the penalized likelihood  
$$l_{pen}(\xi,\gamma, \lambda) =  \log L(\xi, \gamma) + \lambda \int\limits_{x_{min}}^{x_{max}} \left[ f''(x)\right]^2dx,$$
 with $\lambda$ as the penalty term, which controls the intensity of the penalty, and the penalty itself being an integral over the curvature, i.\,e., wiggliness or squared second derivative of the fit. As the second derivative of the fit is often difficult to determine, a penalty matrix $K$ can be constructed which penalizes the curvature as well leading to 
$$l_{pen}(\xi,\gamma, \lambda) = \log L(\xi, \gamma) + \lambda \xi^T K \xi.$$
\begin{figure}[b]
\begin{center}
\vspace{2em}
\includegraphics[width=0.62\textwidth]{Images/wiggly_report}
\end{center}
\vspace{-2em}
\caption[caption]{Bias-variance trade off in flexible regression fits: low bias/high variance (dark grey), high bias/low variance (light grey)}\label{wiggly}
\end{figure}



The choice of $K$ depends on the spline used. In the case of B-splines, first order differences, with $\xi^TK\xi = \sum(\xi_{k+1} - \xi_{k})^2$, are a popular choice. These exploit the fact that using the same value for consecutive $\xi_j$ will result in a horizontal line representing the maximal penalty. First order differences penalize how much each $\xi_j$ can deviate from the one before, therefore controlling the wiggliness of the fit.


Using a penalized regression spline means the wiggliness of the fit can be easily controlled by adjusting the parameter $\lambda$. The remaining problem is how do we determine $\lambda$? This is where mixed models come into play. If we can represent a penalized regression spline as a mixed model, we can use its inference methods to derive an optimal $\lambda$.





\section{Linear Mixed Models}\label{lmm}

Linear mixed models are an extension of the linear model $y_i = x^T_i\beta + \epsilon_i$ by incorporating so called \textit{random effects} in order to capture certain correlation structures in the data \cite{fahrmeir2013regression}. These correlation structures may stem from repeated measurements in longitudinal models or clustered data in hierarchical models. In the estimation procedure, fixed or population-averaged effects are modelled separately from random or individual-specific effects, or cluster-specific effects, respectively. The \textit{measurement model} is written as follows:

$$y_i = X_i \beta + Z_i b_i + \epsilon_i,$$

where $i$ refers to an individual in a longitudinal model and to a cluster in a
hierarchical model, $y_i = ( y_{i1},...,y_{in_i} )^T$ is the $n_i \times 1$ vector of responses for the $n_i$ observations for cluster/individual $i$, $X_i = \left[ x_{i1},..., x_{in_i}\right]^T$ is the $n_i \times p$ matrix of the $p$ fixed-effects covariates, $\beta = (\beta_1,...,\beta_p)^T$ is the $p\times1$ vector of parameters for the fixed effects, $Z_i = \left[z_{i1},...,z_{in_i}\right]^T$ is the $n_i\times q$ matrix of $q$ random covariates, $b = (b_1,...,b_q)^T$ is the $q\times1$ vector of parameters for the random effects, and $\epsilon_i = (\epsilon_{i1},...,\epsilon_{in_i})^T$ is the $n_i\times1$ vector of errors. 

A linear mixed model can be defined hierarchically in stages: 
In the \textit{first stage}, the response is assumed to depend linearly on fixed and random effects as in the measurement model. Additionally, error terms are assumed to be independent and identically distributed (i.i.d.) $\epsilon_i \sim \mathrm{N}_{n_i}(0,R_i)$ typically with $R_i=\sigma^2 I_{n_i}$. 
In the \textit{second stage}, a distribution is introduced for the random effects reflecting the assumption that individuals or clusters are drawn from a bigger population.
This distribution can be seen as a Bayesian prior. It is convention to use Gaussian random effects $b_i \sim \mathrm{N}_q(0,D)$, with $(q{+}1)\times (q{+}1)$ covariance matrix $D$. In addition, $b_i$ and $\epsilon_i$ are assumed to be mutually independent. 


The maximum likelihood estimates for the coefficients are $\hat{\beta} = (X^T \hat{V}^{-1} X)^{-1} X^T \hat{V}^{-1}y$ and $\hat{b} = \hat{D}Z^T\hat{V}^{-1} (y-X\hat{\beta})$, with the covariance matrix of observations $y_i$ in cluster $i$ as $V_i = \mathrm{Cov}(y_i) = Z_iDZ_i^T + R_i$ and the covariance matrix for all observations $V = \mathrm{Cov}(y) = \mathrm{diag}(V_1,...,V_n)$.
The covariance matrix $V$ is unknown as $R_i$ and $D$ are unknown. $V$ can be estimated  using maximum likelihood (ML), if $\beta$ or its estimate $\hat{\beta}$ is known. An alternative approach is to use the restricted  maximum likelihood  (REML). REML is typically used in linear models in order to obtain unbiased estimates. However, it does not generally produce unbiased estimates for mixed models. Nevertheless, it is the more popular of the two methods.  A comparison of ML and REML methods for estimating the variance components including their advantages and disadvantages will be given in Section \ref{varianceestimates}.  The maximization can be done in both cases using Newton-Raphson or Fisher-Scoring algorithms. 

There are four ways of interpreting a mixed model:
\begin{itemize}
\item \textit{classical view}: The random effects reflect that the individuals/clusters are a random sample of a larger population.
\item \textit{marginal view}: The random effects generate a general linear
model with correlated errors.
\item \textit{Bayesian view}: The random effects distribution is effectively a prior on the random effects.
\item \textit{penalization view}: The random effects distribution results in a penalty on the random effects under least squares and maximum likelihood estimation inducing shrinkage towards zero. The intensity of shrinkage decays with $n$.
\end{itemize}


Several linear mixed models can be differentiated:
Whereas the \textit{random coefficient model} depicted above estimates both slope and intercept, for the \textit{random intercept model} $Z_ib_i$ is replaced by a single random deviation per cluster/individual from the population intercept $b_i \in \mathbb{R}$. The \textit{variance components model} assumes that the components of $b_i$ are independent resulting in a diagonal matrix $D$.
\textit{Multilevel mixed models} have more than one cluster affiliation for each observation and \textit{nested mixed models} represent a  subgroup of these, which include hierarchical cluster affiliations.


\section{Semiparametric Models As Mixed Models}\label{representation} % 4 Seiten


The last chapters already fleshed out the idea of representing additive models as mixed models in order to benefit from mixed model inference techniques especially with regard to the penalty term $\lambda$. In order to explore this idea further, an empirical Bayes estimation for an additive model is conducted in the following \cite{kneib2006mixed}. This approach will yield more insights into the idea, but will ultimately be shown to be flawed. Afterwards, a correct way of translating an additive model into a mixed model will be depicted and the resulting penalized likelihood discussed.

\subsection{Naive Empirical Bayes Approach}

Drawing from the introductory example of the truncated power basis, we will attempt to view the additive model $y = \sum_{j=1}^p V_j \xi_j + U \gamma + \epsilon$ with $U$ as the fixed effects and $V_j$ as the random effects. It follows that the prior for $\gamma$ is constant, i.\,e., $p(\gamma) \propto 1$, and the prior for $\xi_j$ is Gaussian (as per convention in mixed models) with a known precision matrix $\Sigma^{-1}_j$ and an unknown scaling parameter $\tau^2_j$ leading to $p(\xi_j|\tau_j^2)  \propto \exp\left(-\frac{1}{2\tau^2_j} \xi_j^T\Sigma^{-1}_j \xi_j\right) $. The posterior is then proportional to the product of the likelihood and the priors
$$p(\xi_1, ... \xi_p, \gamma|y) \propto L(y,\xi_1, ... \xi_p, \gamma)\prod_{j=1}^p p(\xi_j|\tau_j^2). $$

In the next step, the empirical Bayes procedure diverges from a typical Bayes approach by estimating $\tau^2 = (\tau^2_1,..., \tau^2_p)$, which has so far been treated as fixed, via maximum likelihood using the formula
$$\tau^2 = \text{arg}\max_{\tau_1, ..., \tau_p} \log L(y, \xi_1,...,\xi_p, \gamma) - \sum_{j=1}^p \frac{1}{2\tau^2_j} \xi_j^T \Sigma^{-1}_j \xi_j.$$
This equation shows that the empirical Bayes approach is equivalent to penalized maximum likelihood. This result is not surprising as the equivalence of priors and penalties in estimation has already been addressed in Section \ref{lmm}. However, it can be seen here, that the term $\frac{1}{2\tau^2_j}$ is equivalent to the penalty term $\lambda$ and the precision matrix $\Sigma^{-1}_j$ is equivalent to the penalty matrix $K_j$ from the penalized regression splines model formulation in Section \ref{prs}. 

On a first glance this finding seems great as it provides a simple way of choosing the prior in a way that depends on which penalty matrix we chose and also provides a simple way of varying the intensity of the penalization  by adjusting $\tau^2$. However, using the penalty matrix $K_j$ as a precision matrix for the Gaussian prior is problematic because $K_j$ is often rank deficient. For example, in the case of B-splines with first order differences $\xi^TK\xi = \sum(\xi_{k+1} - \xi_{k})^2$. As only differences are penalized, the rank of the penalty matrix is always exactly one lower than full rank. This implies that the Gaussian prior of $\xi_j$ is improper.

\subsection{Decomposing The Non-Linear Effects}

As the approach before demonstrated by leading to an improper prior, an additive model can not be represented as a mixed model by simply treating the linear effects as fixed effects and the non-linear effects as random effects. However, there is a different way of separating the linear and non-linear effects into fixed and random effects which is detailed in Kneib (2006) \cite{kneib2006mixed}. Within this approach, $\gamma$ is still not penalized and therefore $U$ part of the fixed effects, but $\xi_j$ is separated into 
$$\xi_j = \tilde{X}_j \beta_j + \tilde{Z}_j b_j,\label{decomp}$$
with $\beta_j$ expressing the unpenalized parts of $\xi_j$ with a flat prior and dimensionality 
$\text{dim}(\beta_j) = \text{dim}(\xi_j) {-} \text{rank}(K_j)$ and $b_j$ representing the penalized parts of $\xi_j$ with a proper (Gaussian) prior and dimensionality $\text{dim}(b_j) =  \text{rank}(K_j)$.

Figure \ref{priorispenalty} shows how $\beta_j$ and $b_j$ are estimated differently. As $\beta_j$ is not penalized, the mode of the marginal likelihood of $\beta_j$ is its maximum likelihood estimate. On the other hand, $b_j$ is penalized by a prior which is centered around zero and therefore induces shrinkage as the posterior moves toward zero. How much shrinkage is induced depends on the variance of the prior: a low variance leads to more shrinkage and a high variance to less. This is another way of carving out the inverse relationship of the penalty term $\lambda$ and the prior variance $\tau^2_j$.



Now that decomposing $\xi_j$ in this way has been suggested, it is left to show how this decomposition leads to the representation of the additive model equation as the mixed model equation and to prove that this is indeed possible for all penalized regression splines.

For this we start with the typical additive model formulation in Equation \ref{am}. Plugging in 
$\xi_j = \tilde{X}_j \beta_j + \tilde{Z}_j b_j$ leads to Equation \ref{einsetzen}. Now we need to define the components of a mixed model from the parts in this equation. We define $\beta := (\beta_1^T,..., \beta_p^T, \gamma^T)$ and $X := (V_j \tilde{X}_j, U)$, therefore grouping all the unpenalized components together, as well as $b := (b_1^T,...,b_p^T)$ and $Z := V_j \tilde{Z}_j$, grouping all penalized components together. This leads to the familiar mixed model formula in Equation \ref{mm}. 

\begin{figure}[t]
\vspace{2em}
\centering
\hspace{-5.5em}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{Images/priorispenalty_lowvariance_report}
  %\hspace{2em}
\end{minipage}%
\hspace{-2em}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{Images/priorispenalty_highvariance_report}
\end{minipage}
\vspace{-1em}
\caption[caption]{A plot of an unpenalized likelihood depending on a one-dimensional unpenalized $\beta_j$ and a one-dimensional penalized $b_j$, depicting the influence of different priors (low variance on the left and high variance on the right) on the posterior and therefore the penalized maximum likelihood estimates}\label{priorispenalty}
\vspace{1em}
\end{figure}


\begin{align}
y &= \sum_{j=1}^p V_j \xi_j + U\gamma + \epsilon\label{am}\\
 &= \sum_{j=1}^p V_j(\tilde{X}_j \beta_j + \tilde{Z}_j b_j) + U\gamma + \epsilon\label{einsetzen}\\
 &=   X\beta + Zb + \epsilon\label{mm}
\end{align}

In order for this decomposition to result in a mixed model, several requirements need to be fulfilled:

\begin{enumerate}
\item The decomposition is a 1-to-1 transformation. This is equivalent to the matrix $(\tilde{X}_j \:\:  \tilde{Z}_j)$ having full rank. \label{1}
\item The matrices $\tilde{X}_j$ and $\tilde{Z}_j$ are orthogonal, which can be expressed as $\tilde{X}_j^T \tilde{Z}_j = 0$. \label{2}
\item $\beta_j$ is not penalized by $K_j$, i.\,e., $\tilde{X}_j^T K_j \tilde{X}_j = 0$. This is equivalent to $\beta_j$ having a constant prior. \label{3}
\item $b_j$ is penalized and has a proper Gaussian prior, i.\,e., $\tilde{Z}_j^T K_j \tilde{Z}_j = I_{k_j}$, 
where $k_j := \text{rank}(K_j)$. This leads to a proper prior instead of an improper one, which is the goal of this decomposition. \label{4}
\end{enumerate}

These requirements can be fulfilled for all possible penalized regression splines, if $\tilde{X}_j$ and $\tilde{Z}_j$ are constructed in the following way: Condition \ref{3} can easily be fulfilled by setting $\tilde{X}_j$ as a basis of the null space of $K_j$. Choosing $\tilde{Z}_j =L_j (L_j^T L_j)^{-1}$ with $K_j =L_j L_j^T$ satisfies conditions \ref{1} and \ref{4}.  Condition \ref{2} can then be fulfilled by choosing $L_j$ such that $L_j^T\tilde{X}_j = 0$ and $\tilde{X}_jL_j^T = 0$.

 
In order to find a helper matrix $L_j$, spectral decomposition can be utilized, that is, $K_j$ is decomposed into $K_j = \Gamma_j \Lambda_j \Gamma^T$, where $\Gamma_j$ is the matrix of eigenvectors and $\Lambda_j$ the diagonal matrix of eigenvalues of $K_j$, both sorted by the eigenvalues in descending order. After performing this decomposition, $L_j$ can be chosen as $L_j = \Gamma \Lambda_j^{1/2}$, which ensures that all requirements are fulfilled. Note, that $L_j$ is not unique and the calculations presented above provide just a general way of finding a possible $L_j$. It is not necessarily the most efficient way of finding a possible helper matrix. For example, for B-splines with difference matrices as a penalty, e.\,g., the first order differences discussed before, decomposing the penalty matrix $K_j$ into $L_j L_j^T$ is very simple as the first order differences utilize a so called difference matrix $D_j$ which is squared in order to compute the penalty matrix $K_j$, which means it fulfills the requirement $K_j = D_jD_j^T$  automatically. Other penalized regression splines may come with different ways to derive the helper matrix $L_j$ in a more computationally efficient way.

\subsection{Deriving The Penalized Likelihood}\label{likelihood}

After showing how the decomposition is done and that it, in fact, works for all penalized regression splines, the next step is to examine the resulting penalized likelihood. Using the requirements from above (which we know are fulfilled), we can simplify the log-prior as follows:


\begin{align*}
\log p(\xi_j|\tau_j^2)  &\propto -\frac{1}{2\tau^2_j} \xi_j^TK_j \xi_j\\ 
&\overset{\text{Eq.}\:\ref{decomp}}{=} -\frac{1}{2\tau^2_j} (\tilde{X}_j \beta_j + \tilde{Z}_j b_j)^TK_j (\tilde{X}_j \beta_j + \tilde{Z}_j b_j) \\
&= -\frac{1}{2\tau^2_j} (\tilde{X}_j \beta_j)^TK_j (\tilde{X}_j \beta_j) 
-\frac{1}{2\tau^2_j} (\tilde{Z}_j b_j)^TK_j (\tilde{Z}_j b_j)\\
&= -\frac{1}{2\tau^2_j} \beta_j^T \tilde{X}_j^T K_j \tilde{X}_j \beta_j 
-\frac{1}{2\tau^2_j} b_j^T \tilde{Z}_j^T K_j \tilde{Z}_j b_j\\
&\overset{\text{Req.}\:\ref{3},\ref{4}}{=} -\frac{1}{2\tau^2_j} \beta_j^T \cdot 0 \cdot \beta_j 
-\frac{1}{2\tau^2_j} b_j^T \cdot I_{k_j} \cdot b_j\\
&= -\frac{1}{2\tau_j^2} b_j^T b_j.
\end{align*}


Therefore, $\beta$ has a constant prior or respectively no prior assumption on its distribution and $b_j$ has a proper prior with $b_j \sim N(0,\tau_j^2I_{k_j})$. The log-posterior is equivalent to the log-likelihood with

$$l_{pen}(\beta,b|y) = l(y,\beta,b) - \sum_{j=1}^p \frac{1}{2\tau_j^2} b_j^T  b_j. $$

This shows again, that the penalty term $\lambda$ is inversely related to the scaling parameter of the prior $\tau^2$, i.\,e., $\lambda \propto \frac{1}{\tau^2}$. Another fact that can be seen from the equation and the prior distribution is that with this approach a variance components model, meaning that the components of $b_j$ are independent, is estimated as the precision matrix is given with $I_{k_j}$. Therefore, the only parameters that needs to be estimated for the prior are $\tau^2_1$  to $\tau^2_p$.

The decomposition shown in this chapter is an illustrative way of explaining the connection between additive models and mixed models and how one can be represented as the other. However, it is not necessary in order to achieve this representation. A computationally more efficient way is to work with a joint distribution for $\beta_j$ and $b_j$, and thus the unseparated $\xi_j$. This approach is carried out in Wood (2011) \cite{wood2011fast}. However, since the mathematical way of deriving a likelihood and the estimates is mathematically rather complex, it is less suitable for a clear introduction into the topic and the more intuitive way of decomposing $\xi_j$ has been presented here.







\section{Inference} % 3 Seiten

Mixed model inference is not the focus of this report, but it is at the core of why the decomposition explained in the chapter before is useful. Therefore it will be discussed here, although rather briefly. Representing additive models as mixed models is especially practical in determining the penalty term $\lambda$ or, respectively, the variance component $\tau^2$, but it can also be used to derive the other parameters, namely $\beta$, $b$, and $\sigma^2$, rendering any computations under the additive model representation obsolete in estimation \cite{fahrmeir2013regression, kneib2006mixed}. 

\subsection{Estimates For Coefficients}

The estimates for $\beta$ and $b$ are derived by maximizing the log-posterior, which is equivalent to the maximum likelihood approach. Both estimates can be estimated simultaneously based on known variance components $\sigma^2$ and $\tau^2$. For this, the mixed model equations, also known as Henderson equations, 

\begin{align}
\begin{pmatrix}
X^TWX & X^TWZ \\
Z^TWX & Z^TWZ + Q^{-1}
\end{pmatrix}
\begin{pmatrix}
\beta\\
b
\end{pmatrix} =
\begin{pmatrix}
X^TWy \\
Z^TWy
\end{pmatrix},\label{henderson}
\end{align}

 with $W=\text{diag}(\sigma^2)$ and $Q=\text{blockdiag}(\tau_1^2I_{k_1},..., \tau_p^2I_{k_p})$, can be used. The left matrix of this equation represents the Fisher information. The equation resembles the Fisher-Scoring formula, however, it can be computed directly here and no approximation is needed.
 
 \subsection{Estimates For Variance Components}\label{varianceestimates}

As the computation of the estimates for $\beta$ and $b$ depends on the variance components $\sigma^2$ and $\tau^2$, these need to be determined first. This can be accomplished using either maximum likelihood or restricted maximum likelihood. 

In the \textit{maximum likelihood (ML)} approach, $b$ is integrated out of the likelihood derived in Section \ref{likelihood}, assuming the (partially) marginal distribution $y \sim \mathrm{N}(X\beta,\Sigma)$. Additionally, an estimate for $\beta$ is calculated using Equation \ref{henderson}. The resulting $\hat{\beta}$ only depends on $\sigma^2$ and not $\tau^2$. Plugging $\hat{\beta}$ into the log-likelihood  produces the profile log-likelihood for $\tau^2$ and $\sigma^2$, which can then be maximized numerically. ML has the advantage over REML, that it can be used to compare models, which include different smooth terms \cite{wood2011fast}. Using different smooth terms in REML estimation would change the fixed effects structure and render the models incomparable.

For the \textit{restricted maximum likelihood (REML)} approach, $\beta$ is integrated out of the likelihood in addition to $b$, using the marginal distribution of $y$. The resulting likelihood can again be maximized numerically.
While the ML approach estimates the variance components of the posterior mode, REML estimates the mode of the marginal posterior for the variance components. This is a major advantage of the REML approach from a Bayesian viewpoint. Occupying a frequentist point of view, REML as the advantage of taking the loss of the degrees of freedom due to the estimation of $\beta$ into account, which the ML approach does not.

Both approaches need to be maximized numerically, which is typically done using either Fisher-Scoring or Newton-Raphson. Since Newton-Raphson is currently the favoured method, it will be illustrated here using Figure \ref{newton}. In order to maximize the log-likelihood, the root of its derivative, the score function $s(\theta)$, is computed numerically, with $\theta$ as the parameter(s) used to maximize the likelihood. In this case $\theta$ consists of the variance components $\tau^2$ and $\sigma^2$. 
Newton-Raphson produces the root of the score function as follows:

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.55\textwidth]{Images/newton_report}
\end{center}
\vspace{-2em}
\caption[caption]{Newton-Raphson procedure with the initial starting point in the upper right corner}\label{newton}
\vspace{1em}
\end{figure}

\begin{enumerate}
\item Set $t=0$ and initialize $p_t$ with a starting point.
\item Calculate the root of the local derivative of the score function at point $p_{t}$,  use it as the new point $p_{t+1}$, and set $t=t+1$. \label{locderiv}
\item Repeat step \ref{locderiv} until convergence and return the current $p_t$.
\end{enumerate}



\subsection{Nested Iteration Procedure}\label{nested}

Now that the approaches of calculating the coefficients $\beta$ and $b$ as well as the variance components $\tau^2$ and $\sigma^2$ have been presented, this section will discuss how these calculations can be integrated into a single algorithm producing all estimates \cite{wood2011fast}. Until a decade ago this was done using a single iteration method alternating between updating the coefficients $\hat{\beta}$ and $\hat{b}$ given the current $\hat{\tau}^2$ and in turn updating $\hat{\tau}^2$ based on the current $\hat{\beta}$ and $\hat{b}$ using Fisher-Scoring. As both steps depended on the result of the other, convergence could not be guaranteed and this indeed led to convergence problems in a non-negligible amount of cases.

An efficient solution to these convergence problems was introduced by Wood (2011) \cite{wood2011fast}. They suggested a nested iteration setup, updating the estimates for the coefficients and variance components as follows:



\begin{enumerate}
\item Initialize $\hat{\beta}$, $\hat{b}$ and $\hat{\tau}^2$.
\item Update $\hat{\tau}^2$ using Newton-Raphson, while estimating $\hat{\beta}_{\tau}$ and $\hat{b}_\tau$ for each step of the Newton-Raphson algorithm. \label{taustep}
\item Update $\hat{\beta}$ and $\hat{b}$ given the current $\hat{\tau}$.\label{parastep}
\item Alternate between \ref{taustep} and \ref{parastep} until convergence and return the current $\hat{\beta}$, $\hat{b}$ and $\hat{\tau}^2$.
\end{enumerate}

With this approach, updating the variance component $\tau^2$ does not depend on the current estimates of $\beta$ and $b$ but only on the estimates $\hat{\beta}_{\tau}$ and $\hat{b}_\tau$, which are calculated for each Newton-Raphson step in step \ref{taustep}. As a result, this nested iteration method is guaranteed to converge (under mild regulatory conditions).

While the estimate of the variance component $\hat{\sigma}^2$ can be determined analogously to $\hat{\tau}^2$ using this approach, it does not require these rather complex methods and is usually calculated separately.

\section{Comparison To Other Methods} % eine Seite

Representing additive models as mixed models provides a way of using (restricted) maximum likelihood inference in order to determine the optimal penalty term $\hat{\lambda}$. However, other methods to determine an estimate for $\lambda$ were developed as well. The advantages and disadvantages of these will be stated in the following.

\subsection{Fully Bayesian Approach}

Instead of using the restricted maximum likelihood, which is equivalent to empirical Bayes, a fully Bayesian approach based on Markov Chain Monte Carlo (MCMC) inference can be conducted \cite{kneib2006mixed}. For this method, no reparameterization is needed, which enables direct inference for the coefficients $\gamma$ and $\xi_1,...,\xi_p$. 
As the $\tau_j^2$ are not treated as fixed anymore, a hyperprior needs to be chosen. The canonical choice for this hyperprior is an inverse Gamma distribution with $\tau^2_j \sim \text{IG}(0.001,0.001)$. However, this choice might fail, so it is advantageous to calculate the model several times with different parameters for the inverse Gamma distribution in order to obtain a good model. The difficulty of choosing a suitable prior is one of the main disadvantages of the fully Bayesian approach. Another one is that when drawing samples from the posterior using MCMC simulation, it is difficult to determine when the Markov Chain has reached convergence.


\subsection{Prediction Error Methods}

Prediction error methods are all methods, that directly aim at minimizing the prediction error of the model, optimizing, for example, using the Akaike information criterion (AIC) or cross validation (CV)/generalized cross validation (GCV) \cite{wood2011fast}. This leads to better prediction error performance compared to the ML and REML methods discussed in this report. However, all three methods only penalize overfit weakly, which results in a less pronounced optimum in the criterion. As a consequence, prediction error methods have a worse resistance to overfitting, a higher smoothing parameter variability, which is also linked to a slower convergence of these parameters, and an increased tendency to having multiple minima in the criterion to optimize. In the past these methods had an advantage, because they did not have the convergence problems of ML and REML methods. However, since the algorithm presented in Section \ref{nested} was published, they have become less popular.


\section{Conclusion} % halbe Seite

This report showed how additive models can be represented as mixed models in order to benefit from mixed model inference, especially with regard to determining the optimal penalty term $\hat{\lambda}$. 
For this, the coefficients of the non-linear effects $\xi_j$ from the additive model were decomposed into penalized and unpenalized parts. While this provides an intuitive understanding of how these two model classes are 
connected, it is not typically how estimates are calculated computationally, but a more complex prior is assumed for the whole $\xi_j$ and the calculations that follow are adjusted for this fact. However, the approach, where the coefficients of the non-linear effects are separated into unpenalized components with flat priors and penalized components with proper priors, shows the nature of the connection between the penalty and the prior distribution more directly, especially with regard to the penalty matrix.

Using this approach, the report sheds light on the inverse relationship between the penalty term and the prior variance, $\lambda \propto \frac{1}{\tau^2}$, from multiple angles. On the one hand, it was shown how the prior variance and the penalty term influence the likelihood equation in a similar way for additive and mixed models, respectively. On the other hand, it was illustrated that the intensity of shrinkage induced by the penalty is controlled by the variance of the prior. It was also discussed that using a nested iteration setup, that alternates between estimating the coefficients and the variance components, but makes the estimation of the variance components independent of the current coefficient estimates, provides an efficient way to ensuring convergence of the estimation procedure.

This work focused on simple penalized regression splines. However, expressing additive models as mixed models is also possible for other splines, for example, thin plate regression splines \cite{wood2011fast}. It can also be extended to representing generalized additive models or GAMs as generalized linear mixed models \cite{kneib2006mixed}.

The field of flexible regression is extended by the field of mixed models, as it provides a very interesting new perspective on additive models. This is especially relevant in inference, but understanding the link between the two models can enhance one's understanding of either modelling technique beyond that.




\newpage
\printbibliography

\end{document}